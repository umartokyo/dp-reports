## Research Question
To what extent does a deep reinforcement learning agent trained solely through self-play, similar to AlphaZero compare to traditional heuristic-based search algorithm such as Stockfish in their performance, win rate against each other?
## Introduction
Not long ago, Large Language Models (LLM's) such as ChatGPT emerged, demonstrating the capabilities of Artificial Intelligence (AI) across a wide range of language-based tasks (11). However, as major AI labs have been increasing the training data and size of these models, the performance gains appear to be plateauing (10). One of the core reasons is thought to be the reliance of these models on human-generated data: they learn to generalise from patterns of our behaviour, thus they are limited to human performance (12). Interestingly, this pattern of stagnation is not new; a similar bottleneck was once met in another domain: computer chess. (13)

Traditional chess engines based on the rule-based search algorithms such as heuristic evaluation functions with alpha-beta pruning proved successful in defeating the world champion of the time Garry Kasparov and kept improving ever since (14). Although the strength of these engines exceeded human performance by a large margin, progress was limited by the expertise of human experts who've expressed their strategies through careful manual tuning of parameters (4). Additionally, attempts to introduce a deep learning approach failed as engines built with this paradigm, while successful, struggled to match higher-end classical chess engines, with speculations that this is due to the fact they were trained on human gameplay data. (15)

In 2016, DeepMind introduced AlphaGo - an AI agent which learned to play the game of Go not by mimicking human play, but training entirely through self-play. (16) Go is considered to be a complex game with significantly more possible moves on every turn than atoms in the universe! A major breakthrough of this approach was that AlphaGo has beaten the world champion Lee Sedol in the game which was long believed to require intuition and impossible to brute-force. (16)

After AlphaGo, DeepMind developed AlphaZero, a general-purpose reinforcement learning agent capable of mastering any strategy based on two-player games that can be expressed digitally. (7) When AlphaZero was put against Stockfish, the strongest chess engine of the time, in a closed match with 100 games, AlphaZero decisively defeated Stockfish with 28 wins, 72 ties and not a single loss. (8)

AlphaZero achieved, *tabula rasa*, superhuman performance, discovering a new way machines can learn. Instead of relying on human data, being a self-play agent, it surpassed human intelligence entirely, discovering strategies that were previously unimaginable and sometimes incomprehensible by humans. (8) Potentially, if mathematics of AlphaZero can be generalised beyond games to domains such as reasoning or language to go beyond human-level performance, it would be a major step towards superhuman intelligence in the future.

This paper explores the extent to which self-trained reinforcement learning agents like AlphaZero can outperform traditional heuristic search-based systems such as Stockfish. By analysing underlying algorithms and match outcomes of both engines, this paper will seek the understanding of the strengths and limitations of each approach.
## Background Information
### Chess
Chess is a classical two player, perfect information game: meaning both players have complete information of the game at all times. It is also deterministic, with no element of chance involved; each move leads to a predictable outcome. (8) It is played on an 8x8 board with each player controlling one of their own 16 pieces of 6 different types (king, queen, rook, bishop, knight, pawn) with each piece having its own way to move and capture. Players alternate moves, with the main objective being to win by checkmating the opponent (king is under attack and no legal moves to escape the attack exist). Draws are possible when either: insufficient material is left in the game, both players repeat moves 3 times, or a player can't move on their turn. (21)